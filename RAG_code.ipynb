{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c54f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "    and an optional parameter k used in the RRF formula\"\"\"\n",
    "\n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = doc.page_content\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (doc, score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3tecv1hi17m",
   "metadata": {},
   "source": [
    "# 向量存储实现\n",
    "\n",
    "这个单元格实现了SimpleVectorStore类，它是一个基于NumPy的轻量级向量存储系统，支持文档添加和相似性搜索功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee3b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    一个使用 NumPy 的轻量级向量存储实现。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimension=1536):\n",
    "        \"\"\"\n",
    "        初始化向量存储。\n",
    "        参数:\n",
    "            dimension (int): 嵌入向量的维度\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vectors = []  # 存储向量数据\n",
    "        self.documents = []  # 存储文档内容\n",
    "        self.metadata_list = []  # 存储元数据\n",
    "\n",
    "    def add_documents(self, documents, vectors=None, metadata_list=None):\n",
    "        \"\"\"\n",
    "        向向量存储中添加文档。\n",
    "        参数:\n",
    "            documents (List[str]): 文档分块列表\n",
    "            vectors (List[List[float]], 可选): 嵌入向量列表\n",
    "            metadata_list (List[Dict], 可选): 元数据字典列表\n",
    "        \"\"\"\n",
    "        if vectors is None:\n",
    "            vectors = [None] * len(documents)\n",
    "        if metadata_list is None:\n",
    "            metadata_list = [{} for _ in range(len(documents))]\n",
    "        for doc, vec, metadata in zip(documents, vectors, metadata_list):\n",
    "            self.documents.append(doc)\n",
    "            self.vectors.append(vec)\n",
    "            self.metadata_list.append(metadata)\n",
    "\n",
    "    def search(self, query_vector, top_k=5):\n",
    "        \"\"\"\n",
    "        搜索最相似的文档。\n",
    "        参数:\n",
    "            query_vector (List[float]): 查询嵌入向量\n",
    "            top_k (int): 返回的结果数量\n",
    "        返回:\n",
    "            List[Dict]: 包含文档、相似度分数和元数据的结果列表\n",
    "        \"\"\"\n",
    "        if not self.vectors or not self.documents:\n",
    "            return []\n",
    "        # 将查询向量转换为 NumPy 数组\n",
    "        query_array = np.array(query_vector)\n",
    "        # 计算相似度\n",
    "        similarities = []\n",
    "        for index, vector in enumerate(self.vectors):\n",
    "            if vector is not None:\n",
    "                # 计算余弦相似度\n",
    "                similarity = np.dot(query_array, vector) / (\n",
    "                    np.linalg.norm(query_array) * np.linalg.norm(vector)\n",
    "                )\n",
    "                similarities.append((index, similarity))\n",
    "        # 根据相似度排序（降序）\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        # 获取 top-k 结果\n",
    "        results = []\n",
    "        for index, score in similarities[:top_k]:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"document\": self.documents[index],\n",
    "                    \"score\": float(score),\n",
    "                    \"metadata\": self.metadata_list[index],\n",
    "                }\n",
    "            )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zsdo3dj1kvm",
   "metadata": {},
   "source": [
    "# 文档处理函数\n",
    "\n",
    "这个单元格包含了处理文档的函数，用于将PDF文档转换为文本块并生成相应的向量表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "def process_document(\n",
    "    pdf_path: str, chunk_size: int = 800\n",
    ") -> Tuple[List[str], SimpleVectorStore, Dict]:\n",
    "    \"\"\"\n",
    "    处理文档以便与 RSE（检索增强生成）一起使用。\n",
    "\n",
    "    参数:\n",
    "        pdf_path(str): PDF 文档的路径\n",
    "        chunk_size(int): 每个文本块的大小（字符数）\n",
    "\n",
    "    返回:\n",
    "        Tuple[List[str], SimpleVectorStore, Dict]: 包含文本块列表、向量存储实例和文档信息的元组\n",
    "    \"\"\"\n",
    "    print(\"从文档中提取文本...\")\n",
    "    # 从 PDF 文件中提取文本内容\n",
    "    document_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"将文本分割为非重叠片段...\")\n",
    "    # 将提取的文本分割为非重叠的文本块\n",
    "    text_chunks = chunk_text(document_text, chunk_size=chunk_size, overlap=0)\n",
    "    print(f\"共创建了 {len(text_chunks)} 个文本块\")\n",
    "\n",
    "    print(\"为文本块生成嵌入向量...\")\n",
    "    # 为每个文本块生成嵌入向量\n",
    "    chunk_embeddings = create_embeddings(text_chunks)\n",
    "\n",
    "    # 创建一个 SimpleVectorStore 实例用于存储向量数据\n",
    "    vector_store = SimpleVectorStore()\n",
    "\n",
    "    # 添加带有元数据的文档（包含文本块索引，用于后续重建）\n",
    "    metadata_list = [\n",
    "        {\"chunk_index\": index, \"source\": pdf_path} for index in range(len(text_chunks))\n",
    "    ]\n",
    "    vector_store.add_documents(text_chunks, chunk_embeddings, metadata_list)\n",
    "\n",
    "    # 记录原始文档结构以供后续拼接使用\n",
    "    document_info = {\n",
    "        \"chunks\": text_chunks,\n",
    "        \"source\": pdf_path,\n",
    "    }\n",
    "\n",
    "    return text_chunks, vector_store, document_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jet6qkn3pnn",
   "metadata": {},
   "source": [
    "# 文档切片价值计算\n",
    "\n",
    "这个单元格实现了计算文档切片价值值的函数，结合相关性得分和位置信息来评估每个文本块的重要性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_values(\n",
    "    query: str, chunks: List[str], vector_store, irrelevant_chunk_penalty: float = 0.2\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    计算每个文档切片的价值值（value），结合其相关性得分和位置信息。\n",
    "\n",
    "    参数:\n",
    "        query(str): 用户输入的查询文本\n",
    "        chunks(List[str]): 文档被切分后的文本块列表\n",
    "        vector_store: 向量数据库，包含文档块的向量表示\n",
    "        irrelevant_chunk_penalty(float): 对无关文档块施加的惩罚值，默认是0.2\n",
    "\n",
    "    返回:\n",
    "        List[float]: 每个文档块对应的价值值列表（浮点数）\n",
    "    \"\"\"\n",
    "\n",
    "    # 将用户查询转换成嵌入向量以进行语义匹配\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "\n",
    "    # 获取所有文本块数量并搜索相似度结果\n",
    "    total_chunks = len(chunks)\n",
    "    search_results = vector_store.search(query_embedding, top_k=total_chunks)\n",
    "\n",
    "    # 构建 chunk_index 到相关性得分的映射字典\n",
    "    relevance_scores = {\n",
    "        result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in search_results\n",
    "    }\n",
    "\n",
    "    # 根据相关性得分计算价值值，并应用不相关块的惩罚机制\n",
    "    chunk_values = []\n",
    "    for i in range(total_chunks):\n",
    "        score = relevance_scores.get(i, 0.0)\n",
    "        value = score - irrelevant_chunk_penalty\n",
    "        chunk_values.append(value)\n",
    "\n",
    "    return chunk_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xfddzpals3e",
   "metadata": {},
   "source": [
    "# 段落重建与格式化\n",
    "\n",
    "这个单元格包含了重建文本段落和格式化上下文的函数，用于将最佳文本块重新组合成连贯的段落并为语言模型生成合适的输入格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_segments(\n",
    "    document_chunks: List[str], best_segment_indices: List[Tuple[int, int]]\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    根据最佳切片索引重建文本段落。\n",
    "\n",
    "    参数:\n",
    "        document_chunks(List[str]): 原始文档的所有文本块\n",
    "        best_segment_indices(List[Tuple[int, int]]): 最佳段落的起始和结束索引列表\n",
    "\n",
    "    返回:\n",
    "        List[Dict]: 包含重建段落及其范围的字典列表\n",
    "    \"\"\"\n",
    "    reconstructed_segments = []\n",
    "\n",
    "    for start_idx, end_idx in best_segment_indices:\n",
    "        segment_text = \" \".join(document_chunks[start_idx:end_idx])\n",
    "        reconstructed_segments.append(\n",
    "            {\n",
    "                \"text\": segment_text,\n",
    "                \"segment_range\": (start_idx, end_idx),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return reconstructed_segments\n",
    "\n",
    "\n",
    "def format_segments_for_context(segments: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    将文本段落格式化为语言模型可用的上下文字符串。\n",
    "\n",
    "    参数:\n",
    "        segments(List[Dict]): 包含段落文本和索引范围的字典列表\n",
    "\n",
    "    返回:\n",
    "        str: 格式化的上下文文本\n",
    "    \"\"\"\n",
    "    context_lines = []\n",
    "\n",
    "    for index, segment in enumerate(segments):\n",
    "        header = f\"SEGMENT {index + 1} (Chunks {segment['segment_range'][0]}-{segment['segment_range'][1] - 1}):\"\n",
    "        context_lines.append(header)\n",
    "        context_lines.append(segment[\"text\"])\n",
    "        context_lines.append(\"-\" * 80)\n",
    "\n",
    "    return \"\\n\\n\".join(context_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sbblh94rogb",
   "metadata": {},
   "source": [
    "# 完整的RAG流程实现\n",
    "\n",
    "这个单元格实现了完整的RAG（检索增强生成）流程，使用相关段落提取（RSE）策略来筛选最有用的文档内容并生成最终回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b196f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rse(pdf_path: str, query: str, chunk_size: int = 800, penalty: float = 0.2) -> Dict:\n",
    "    \"\"\"\n",
    "    完整的 RAG 流程，使用相关段落提取（RSE）策略筛选最有用的文档内容。\n",
    "    \n",
    "    参数:\n",
    "        pdf_path(str): PDF 文档路径\n",
    "        query(str): 用户查询\n",
    "        chunk_size(int): 文本切片大小\n",
    "        penalty(float): 不相关切片的惩罚系数\n",
    "        \n",
    "    返回:\n",
    "        Dict: 包含查询、选中的段落以及生成回答的结果字典\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 开始执行基于相关段落提取的 RAG 流程 ===\")\n",
    "    print(f\"查询内容: {query}\")\n",
    "\n",
    "    # 步骤 1：处理文档并生成向量存储\n",
    "    text_chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "\n",
    "    # 步骤 2：计算每个文本块的相关性得分与价值值\n",
    "    print(\"\\n正在计算文本块相关性得分与价值值...\")\n",
    "    chunk_values = calculate_chunk_values(query, text_chunks, vector_store, penalty)\n",
    "\n",
    "    # 步骤 3：根据价值值选择最优段落\n",
    "    best_segments, scores = find_best_segments(\n",
    "        chunk_values=chunk_values,\n",
    "        max_segment_length=20,\n",
    "        total_max_length=30,\n",
    "        min_segment_value=0.2\n",
    "    )\n",
    "\n",
    "    # 步骤 4：重建最佳段落\n",
    "    print(\"\\n正在重建最佳文本段落...\")\n",
    "    selected_segments = reconstruct_segments(text_chunks, best_segments)\n",
    "\n",
    "    # 步骤 5：格式化上下文供大模型使用\n",
    "    formatted_context = format_segments_for_context(selected_segments)\n",
    "\n",
    "    # 步骤 6：调用大模型生成最终回复\n",
    "    response = generate_response(query, formatted_context)\n",
    "\n",
    "    # 整理输出结果\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"segments\": selected_segments,\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== 最终回复如下 ===\")\n",
    "    print(response)\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibi-service",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
