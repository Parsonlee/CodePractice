{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "# 假设你希望使用 'SimHei'\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'WenQuanYi Zen Hei', 'Arial Unicode MS'] # 添加多个备用字体\n",
    "plt.rcParams['axes.unicode_minus'] = False # 解决负号 '-' 显示为方块的问题\n",
    "plt.rcParams['font.size'] = 12 # 可以适当调整字体大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relevance_scores(query_embeddings, document_embeddings):\n",
    "    \"\"\"\n",
    "    计算给定查询的前 k 个最相关文档。\n",
    "\n",
    "    参数:\n",
    "    query_embeddings: 表示查询嵌入的张量 (tensor),形状: [seq_len, d_model]\n",
    "    document_embeddings: 表示 k 个文档嵌入的张量,形状: [k, max_sql_len, d_model]\n",
    "\n",
    "    返回: 基于相关性分数排序的文档索引\n",
    "    \"\"\"\n",
    "\n",
    "    # 注: 假设 document_embeddings 已经进行了适当的填充并转移到 GPU\n",
    "\n",
    "    # 1. 计算查询嵌入和文档嵌入的批量点积\n",
    "    # scores = torch.matmul(query_embeddings.unsqueeze(0), document_embeddings.transpose(1, 2))\n",
    "    scores = torch.einsum(\"sd, kmd -> ksm\", query_embeddings, document_embeddings)\n",
    "    print(\"scores shape:\", scores.shape)  # 输出形状应为 [k, seq_len, max_sql_len]\n",
    "\n",
    "    # 2. 在文档词语维度上应用最大池化,找出每个查询词语的最大相似度\n",
    "    max_scores_per_query_term = scores.max(dim=2).values\n",
    "    print(\n",
    "        \"max_scores_per_query_term shape:\", max_scores_per_query_term.shape\n",
    "    )  # 输出形状应为 [k, seq_len]\n",
    "\n",
    "    # 3. 对查询词语的分数求和,得到每个文档的总分\n",
    "    total_scores = max_scores_per_query_term.sum(dim=1)\n",
    "    print(\"total_scores shape:\", total_scores.shape)  # 输出形状应为 [k]\n",
    "\n",
    "    # 4. 根据总分对文档进行降序排序\n",
    "    sorted_indices = total_scores.argsort(descending=True)\n",
    "\n",
    "    return sorted_indices\n",
    "\n",
    "\n",
    "# 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置随机种子以保证结果可复现\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # 模拟输入数据\n",
    "    num_queries = 5  # 查询中的词数\n",
    "    embedding_dim = 768  # 假设使用BERT-base的嵌入维度\n",
    "    num_documents = 3  # 测试用文档数量\n",
    "    max_doc_length = 10  # 文档的最大长度\n",
    "\n",
    "    # 随机生成查询和文档嵌入\n",
    "    query_embeddings = torch.randn(num_queries, embedding_dim)\n",
    "    document_embeddings = torch.randn(num_documents, max_doc_length, embedding_dim)\n",
    "\n",
    "    # 计算相关性分数\n",
    "    relevance_scores = compute_relevance_scores(query_embeddings, document_embeddings)\n",
    "\n",
    "    print(\"相关性分数:\", relevance_scores)\n",
    "\n",
    "    # 根据相关性分数对文档进行排序\n",
    "    sorted_indices = relevance_scores.argsort(descending=True)\n",
    "    print(\"按相关性分数降序排列的文档索引:\", sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 创建一个示例的“高秩”矩阵 ---\n",
    "# 我们可以创建一个 M x N 的矩阵\n",
    "M, N = 100, 80\n",
    "# 为了演示，我们先创建一个真实的低秩矩阵，然后加上一些噪声，使其变成一个“高秩”但可近似的矩阵\n",
    "# 创建两个“瘦”矩阵，它们的乘积将是低秩的\n",
    "rank_k = 5 # 假设真实秩为 5\n",
    "U_true = np.random.rand(M, rank_k)\n",
    "V_true = np.random.rand(N, rank_k)\n",
    "\n",
    "# 真实的低秩矩阵\n",
    "original_low_rank_matrix = np.dot(U_true, V_true.T)\n",
    "\n",
    "# 加上一些随机噪声，使其变为一个“高秩”但有内在低秩结构的数据矩阵\n",
    "noise = np.random.randn(M, N) * 0.1 # 噪音强度\n",
    "A_original = original_low_rank_matrix + noise\n",
    "\n",
    "print(f\"原始矩阵 A_original 形状: {A_original.shape}\")\n",
    "print(f\"原始矩阵 A_original 的秩 (理论上，实际计算可能因噪声而接近满秩): {np.linalg.matrix_rank(A_original)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 2. 使用奇异值分解 (SVD) 进行低秩分解 ---\n",
    "# SVD 将矩阵 A 分解为 U * S * Vh\n",
    "# U: 左奇异向量矩阵 (M x M)\n",
    "# s: 奇异值向量 (min(M, N) 长度)\n",
    "# Vh: 右奇异向量的共轭转置矩阵 (N x N)\n",
    "U, s, Vh = np.linalg.svd(A_original)\n",
    "\n",
    "print(f\"U 矩阵形状: {U.shape}\")\n",
    "print(f\"奇异值向量 s 的长度: {s.shape}\")\n",
    "print(f\"Vh 矩阵形状: {Vh.shape}\")\n",
    "print(\"\\n前10个奇异值 (SVD能够揭示矩阵的“能量”或重要性):\")\n",
    "print(s[:10])\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 3. 通过截断 SVD 实现低秩近似 ---\n",
    "# 我们选择一个比原始矩阵秩小得多的 k，来近似原矩阵\n",
    "approx_rank = 5 # 我们希望用秩为5的矩阵来近似\n",
    "\n",
    "if approx_rank > len(s):\n",
    "    approx_rank = len(s) # 确保不超过奇异值数量\n",
    "\n",
    "# 提取前 k 个奇异值、左奇异向量和右奇异向量\n",
    "U_k = U[:, :approx_rank]\n",
    "s_k = np.diag(s[:approx_rank]) # 将奇异值向量转换为对角矩阵\n",
    "Vh_k = Vh[:approx_rank, :]\n",
    "\n",
    "# 重构低秩近似矩阵 A_approx\n",
    "# A_approx = U_k * s_k * Vh_k\n",
    "A_approx = np.dot(U_k, np.dot(s_k, Vh_k))\n",
    "\n",
    "print(f\"低秩近似矩阵 A_approx 的形状: {A_approx.shape}\")\n",
    "print(f\"低秩近似矩阵 A_approx 的秩: {np.linalg.matrix_rank(A_approx)}\") # 理论上是 approx_rank\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 4. 比较原始矩阵和低秩近似矩阵 ---\n",
    "# 计算近似误差（Frobenius范数）\n",
    "approximation_error = np.linalg.norm(A_original - A_approx, 'fro')\n",
    "print(f\"原始矩阵和低秩近似矩阵之间的误差 (Frobenius范数): {approximation_error}\")\n",
    "\n",
    "# 可视化前几个奇异值，可以看到它们衰减得很快，这正是低秩矩阵的特征\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(s, 'o-')\n",
    "plt.title('Singular Values (奇异值)')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.axvline(x=approx_rank - 1, color='r', linestyle='--', label=f'Approximation Rank = {approx_rank}')\n",
    "plt.legend()\n",
    "plt.yscale('log') # 奇异值通常呈指数衰减，用对数坐标更清晰\n",
    "plt.show()\n",
    "\n",
    "# 进一步可视化，展示原始矩阵和近似矩阵的差异（如果矩阵足够小，可以显示图像）\n",
    "# 这里只显示一个小块，因为整个矩阵太大\n",
    "if M * N <= 2500: # 如果矩阵足够小，才尝试显示为图像\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(A_original, cmap='gray', aspect='auto')\n",
    "    plt.title('Original Matrix')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(A_approx, cmap='gray', aspect='auto')\n",
    "    plt.title(f'Low-Rank Approximation (rank={approx_rank})')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n矩阵太大，不适合直接可视化为图像，请检查数值。\")\n",
    "\n",
    "print(\"\\n--- 关键点总结 ---\")\n",
    "print(f\"原始矩阵存储参数量: {M * N}\")\n",
    "print(f\"低秩近似矩阵存储参数量 (U_k, s_k, Vh_k): {M * approx_rank + approx_rank + N * approx_rank} (不包括额外的对角矩阵存储，如果只存向量的话)\")\n",
    "print(f\"当 rank_k ({approx_rank}) 远小于 M({M}) 和 N({N}) 时，参数量大大减少，实现了压缩。\")\n",
    "print(\"例如，如果只存储 U_k 和 Vh_k，参数量为 (M+N)*approx_rank。\")\n",
    "print(f\"本例中：原始参数 {M*N} = {M*N}，近似参数 {(M+N)*approx_rank} = {(M+N)*approx_rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEnhancedModule(nn.Module):\n",
    "    \"\"\"知识推理增强的注意力机制模块\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionEnhancedModule, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 动态融合题目信息和推理过程的门控机制\n",
    "        self.query_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.gate = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        \n",
    "        # 多尺度特征提取\n",
    "        self.scale_transforms = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Linear(hidden_size, hidden_size // 4)\n",
    "        ])\n",
    "        self.scale_combine = nn.Linear(hidden_size + hidden_size // 2 + hidden_size // 4, hidden_size)\n",
    "        \n",
    "        # 归一化层\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x, context=None):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        x: 题目理解的特征 [batch_size, seq_len, hidden_size]\n",
    "        context: 可选的上下文特征 [batch_size, seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "        if context is None:\n",
    "            context = x\n",
    "            \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # 自注意力计算\n",
    "        q = self.query_linear(x)\n",
    "        k = self.key_linear(context)\n",
    "        v = self.value_linear(context)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1, -2)) / (self.hidden_size ** 0.5)\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # 应用注意力\n",
    "        attention_output = torch.matmul(attention_probs, v)\n",
    "        \n",
    "        # 门控机制\n",
    "        gate_input = torch.cat([x, attention_output], dim=-1)\n",
    "        gate_value = torch.sigmoid(self.gate(gate_input))\n",
    "        gated_output = gate_value * attention_output + (1 - gate_value) * x\n",
    "        \n",
    "        # 多尺度特征提取\n",
    "        scales = [gated_output]\n",
    "        for transform in self.scale_transforms:\n",
    "            scales.append(transform(gated_output))\n",
    "        \n",
    "        # 组合多尺度特征\n",
    "        multi_scale = torch.cat([s for s in scales], dim=-1)\n",
    "        combined = self.scale_combine(multi_scale)\n",
    "        \n",
    "        # 残差连接和层归一化\n",
    "        output = self.layer_norm(combined + x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class KnowledgeRoutingModule(nn.Module):\n",
    "    \"\"\"动态知识路由模块\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, num_domains=4):\n",
    "        super(KnowledgeRoutingModule, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_domains = num_domains\n",
    "        \n",
    "        # 知识域专家网络\n",
    "        self.domain_experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_size, hidden_size)\n",
    "            ) for _ in range(num_domains)\n",
    "        ])\n",
    "        \n",
    "        # 路由网络\n",
    "        self.router = nn.Linear(hidden_size, num_domains)\n",
    "        \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        x: 输入特征 [batch_size, seq_len, hidden_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # 计算路由权重\n",
    "        routing_logits = self.router(x.mean(dim=1))  # [batch_size, num_domains]\n",
    "        routing_weights = F.softmax(routing_logits, dim=-1)  # [batch_size, num_domains]\n",
    "        \n",
    "        # 扩展维度以便于广播\n",
    "        routing_weights = routing_weights.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, num_domains]\n",
    "        \n",
    "        # 将输入通过每个专家网络\n",
    "        expert_outputs = []\n",
    "        for expert in self.domain_experts:\n",
    "            expert_output = expert(x).unsqueeze(-1)  # [batch_size, seq_len, hidden_size, 1]\n",
    "            expert_outputs.append(expert_output)\n",
    "        \n",
    "        # 堆叠专家输出 [batch_size, seq_len, hidden_size, num_domains]\n",
    "        stacked_outputs = torch.cat(expert_outputs, dim=-1)\n",
    "        \n",
    "        # 应用路由权重\n",
    "        routed_output = torch.sum(stacked_outputs * routing_weights, dim=-1)\n",
    "        \n",
    "        # 输出层\n",
    "        output = self.output_layer(routed_output)\n",
    "        output = self.layer_norm(output + x)  # 残差连接\n",
    "        \n",
    "        return output, routing_weights.squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4445, 0.5470, 0.6187, 0.6925],\n",
      "        [0.5750, 0.3050, 0.5550, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "Input shapes: x1torch.Size([2, 4]), x2torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GatingMechanism(nn.Module):\n",
    "    \"\"\"\n",
    "    基本的门控机制实现\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super(GatingMechanism, self).__init__()\n",
    "        # 门控网络，输出0-1之间的值\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size * 2, input_size),  # 输入是两个向量的拼接\n",
    "            nn.Sigmoid()  # Sigmoid函数将输出压缩到0-1之间\n",
    "        )\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        x1, x2: 输入张量，形状为 [..., input_size]\n",
    "        \"\"\"\n",
    "        # 拼接两个输入\n",
    "        concat_input = torch.cat([x1, x2], dim=-1)\n",
    "        \n",
    "        # 计算门控值\n",
    "        gate_value = self.gate(concat_input)\n",
    "        print(gate_value)\n",
    "        \n",
    "        # 应用门控机制：g * x1 + (1-g) * x2\n",
    "        output = gate_value * x1 + (1 - gate_value) * x2\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 示例使用\n",
    "input_size = 4\n",
    "gate_module = GatingMechanism(input_size)\n",
    "\n",
    "# 创建示例输入\n",
    "x1 = torch.randn(2, input_size)  # batch_size=2\n",
    "x2 = torch.randn(2, input_size)\n",
    "\n",
    "# 应用门控机制\n",
    "output = gate_module(x1, x2)\n",
    "print(f\"Input shapes: x1{x1.shape}, x2{x2.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM code practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "# 假设你希望使用 'SimHei'\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'WenQuanYi Zen Hei', 'Arial Unicode MS'] # 添加多个备用字体\n",
    "plt.rcParams['axes.unicode_minus'] = False # 解决负号 '-' 显示为方块的问题\n",
    "plt.rcParams['font.size'] = 12 # 可以适当调整字体大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],\n",
    "        [0.55, 0.87, 0.66],\n",
    "        [0.57, 0.85, 0.64],\n",
    "        [0.22, 0.58, 0.33],\n",
    "        [0.77, 0.25, 0.10],\n",
    "        [0.05, 0.80, 0.55],\n",
    "    ]\n",
    ")\n",
    "print(\"batch size 1, seq length 6, dim 3\")\n",
    "inputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = inputs[1].shape[0]\n",
    "\n",
    "w_q = torch.nn.Parameter(torch.rand(d_model, d_model), requires_grad=False)\n",
    "w_k = torch.nn.Parameter(torch.rand(d_model, d_model), requires_grad=False)\n",
    "w_v = torch.nn.Parameter(torch.rand(d_model, d_model), requires_grad=False)\n",
    "\n",
    "x_2 = inputs[1]\n",
    "query_2 = x_2 @ w_q\n",
    "key_2 = x_2 @ w_k\n",
    "value_2 = x_2 @ w_v\n",
    "\n",
    "query_2\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "attn_weights @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_k(x)\n",
    "        values = self.W_v(x)\n",
    "        queries = self.W_q(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[0]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_v1 = SelfAttention_v2(3, 3)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen = 6\n",
    "torch.tril(torch.ones(seqlen, seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = attn_weights.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_len, context_len))\n",
    "attn_weights * mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sum = mask_simple.sum(dim=1, keepdim=True)\n",
    "mask_simple_norm = mask_simple / row_sum\n",
    "mask_simple_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(torch.ones(context_len, context_len), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.softmax(masked / d_model**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(0.5)\n",
    "dropout(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementing compact causal self-attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        mask = torch.triu(torch.ones(num_tokens, num_tokens), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill_(\n",
    "            mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores / d_in**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    \n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(3, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        # Reduce the projection dim to match desired output dim\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # combine heads, where self.d_out = num_heads * head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
